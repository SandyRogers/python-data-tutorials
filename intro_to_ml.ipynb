{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f76c88",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Python\n",
    "\n",
    "This is a Jupyter Notebook, designed to help you understand and use some concepts and libraries for machine learning using the Python coding language and ecosystem.\n",
    "\n",
    "We will be using [scikit‑learn](https://scikit-learn.org/) library for the machine learning parts.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. **Guided classification on the Iris dataset** – walk through a complete machine learning pipeline using a built‑in toy dataset. This is a \"classic\" dataset and example.\n",
    "2. **Unsupervised clustering with k‑means** – understand how to group data without knowing their labels/categories in advance.\n",
    "3. **Self‑guided exercise on an anaemia dataset** – apply similar techniques to a real‑world haematology dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effb4f2-2457-4c0a-bbba-dee10ad4197e",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    " To run each cell of code, type shift-enter, or press the play icon in the toolbar near the top ▶\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35b71d",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before running this notebook you should ensure that the required packages are installed. The main libraries we will use are:\n",
    "\n",
    "- **pandas** for data manipulation\n",
    "- **numpy** for numerical arrays\n",
    "- **seaborn** and **matplotlib** for plotting\n",
    "- **scikit‑learn** for machine learning models and evaluation metrics\n",
    "\n",
    "Run the following cell to import these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b900a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit‑learn imports\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# display settings\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84fb99",
   "metadata": {},
   "source": [
    "## 1. Supervised classification with the Iris dataset\n",
    "\n",
    "To illustrate a complete machine learning workflow we will use the classic **Iris** dataset. This dataset contains measurements of three species of iris flower (setosa, versicolor and virginica). Each sample includes four numerical features:\n",
    "\n",
    "1. *Sepal length*\n",
    "2. *Sepal width*\n",
    "3. *Petal length*\n",
    "4. *Petal width*\n",
    "\n",
    "Our goal is to build a classifier that predicts the species based on these measurements. The steps below walk you through loading the dataset, exploring it, training a model and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b73cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# This is a shortcut to a built-in dataset for tutorials. \n",
    "# A real world equivalent might use something like pandas.read_excel(...) to load tabular/spreadsheet data.\n",
    "\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)  \n",
    "# By the way, in Python variables are usually lowercase... \n",
    "#   but in mathematical Python, many people break that coding convention to use the maths convention of uppercase for matrices.\n",
    "\n",
    "y = pd.Series(iris.target, name='species')\n",
    "\n",
    "print('Shape of feature matrix:', X.shape)\n",
    "print('Number of labels:', y.nunique())\n",
    "\n",
    "# Display the first few rows of the data\n",
    "iris_df = pd.concat([X, y], axis=1)\n",
    "iris_df.head()  # By the way, in Jupyter Notebooks, the last thing in the cell is automatically printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb3703",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "Before building models it is important to explore the data. A quick way to visualise pairwise relationships between numerical features coloured by the class is to use `seaborn.pairplot`. This can highlight how separable the species are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58820a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of features coloured by species\n",
    "sns.pairplot(iris_df, hue='species', diag_kind='hist')\n",
    "plt.suptitle('Pairwise feature relationships in the Iris dataset', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdc1a4-af86-48b4-9629-60478b48b1e4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  Looking at these plots... what do you expect a model to learn about how to the species of an unknown flower?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b23ba4-efa9-444d-a8f3-82dcd3d153ca",
   "metadata": {},
   "source": [
    "My notes...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59d0f4",
   "metadata": {},
   "source": [
    "### Train–test split\n",
    "\n",
    "We will split the dataset into a **training set** and a **test set** so that we can evaluate (with the test set) how well our classifier (which learns from the training set) generalises to novel data. A common practice is to use 70–80% of the data for training and the remainder for testing.\n",
    "\n",
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  Can you think of cases where this may be problematic?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f7812-6737-4c44-a093-bbd7756ca19d",
   "metadata": {},
   "source": [
    "My notes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059aec8-fa36-4397-a965-78865626b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print('Training samples:', X_train.shape[0])\n",
    "print('Test samples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981c30e",
   "metadata": {},
   "source": [
    "### Fitting a Random Forest classifier\n",
    "\n",
    "A **decision tree** in machine learning is just like one you might draw out to explain how to follow a decision process in real life... like:\n",
    "\n",
    "```\n",
    "is petal_length < 2.5?\n",
    " ├── yes → class = setosa\n",
    " └── no → is petal_width < 1.8?\n",
    "        ├── yes → class = versicolor\n",
    "        └── no → class = virginica\n",
    "```\n",
    "\n",
    "This tree repeatedly subsets the data until most data fed to it is given a satisfactory label. Such a tree can be automatically learned from data, using `sklearn` or similar tools.\n",
    "\n",
    "A **random forest** is an ensemble of decision trees; each tree is trained on a bootstrapped subset of the data and randomly selects a subset of features when splitting nodes. Averaging the predictions of many trees often improves accuracy and reduces [over‑fitting](https://en.wikipedia.org/wiki/Overfitting) (where the model is too specific to the training data).\n",
    "\n",
    "Below we instantiate a `RandomForestClassifier`, let's train it on the training data and make predictions on the test data.\n",
    "Those predictions let us examine performance metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c709d83-ee5b-4844-bc00-6d38d2c08555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model (you can adjust n_estimators and other hyperparameters)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit on the training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117adf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291397fd",
   "metadata": {},
   "source": [
    "**Precision, recall and F1**\n",
    "\n",
    "In binary classification the **precision** is the ratio of true positives to all predicted positives, indicating how many selected items were relevant; the **recall** is the ratio of true positives to all actual positives, measuring how many relevant items were selected. \n",
    "\n",
    "The **f1 score** is a weighted harmonic mean of precision and recall. For multi‑class problems scikit‑learn applies these metrics for each class and averages them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452d1bb",
   "metadata": {},
   "source": [
    "### Cross‑validation\n",
    "\n",
    "Random train/test splits can be sensitive to how the data are partitioned. **Cross‑validation** mitigates this by repeatedly splitting the data into different train/test folds and averaging the scores. In scikit‑learn this is done with `cross_val_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10d010-2ca4-4409-b85e-5a0769c82fa9",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  Consider... what could happen if one species was very rare in the dataset, and we only uses a single train/test split?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5‑fold cross‑validation on the training set\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross‑validated accuracy scores:', cv_scores)\n",
    "print('Mean CV accuracy: %.3f' % cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0ad4e-71a6-4cc5-b2b7-86221e7c065a",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  K-fold splitting segments the entire dataset by \"folding\" it as if it were printed on a long piece of paper, e.g. into 5 folds each of which is used to test the predictions in turn. An alternative is \"Monte Carlo\", where e.g. 5 random test/train splits would be made. What advantage(s) does k-fold have over Monte Carlo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b3637",
   "metadata": {},
   "source": [
    "## 2. Unsupervised clustering with k‑means\n",
    "\n",
    "In unsupervised learning there are no labels to learn from. Instead we aim to discover structure in the observed data (e.g. the petal measurements). A widely used clustering algorithm is **k‑means**, which partitions data into k clusters by minimising the within‑cluster sum of squares; it alternates between assigning each datapoint to the nearest centroid and updating centroids to the mean of their assigned points.\n",
    "\n",
    "We'll apply k‑means to the iris features and examine how well the resulting clusters correspond to the actual species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af73840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise features before clustering (important when variables are on different scales)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform k‑means clustering with k=3 (there are three species)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Compare clusters to true labels\n",
    "pd.crosstab(y, clusters, rownames=['Actual species'], colnames=['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b560790-00e2-4635-8cac-61fd17abf422",
   "metadata": {},
   "source": [
    "Although k‑means is unsupervised, in this case it largely recovers the species structure because the iris classes are well separated. You can try varying the number of clusters and initialisation parameters to see how the assignments change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290bb8d2-aaf8-4839-bdaa-42752c41138b",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  What do you think this <a href=\"https://en.wikipedia.org/wiki/Contingency_table\">Cross Tab (contingency table)</a> is telling you? What does it mean about how successfully the model learned the data structure of the species separation?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58755097-7d00-4e91-b935-c3cf463e36ec",
   "metadata": {},
   "source": [
    "My notes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9447f-4c25-4352-8854-2322d6acfe04",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  Bonus! Given that the model is some multi-dimensional model... do you know any techniques to try and visualise how well the clusters have been formed? Hint: people are much better are visualising things in 2 dimensions. And so you'd need to find the 2 principle dimensions of the model...\n",
    "  <details>\n",
    "    <summary>Show me the code!</summary>\n",
    "    <dt>\n",
    "        <code>\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='viridis', s=40)\n",
    "        plt.title(\"K-Means clusters (projected via PCA)\")\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.show()\n",
    "      </code>\n",
    "    </dt>\n",
    "  </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6e31a-95ef-432a-aac1-b1357a781990",
   "metadata": {},
   "source": [
    "My notes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e89d4",
   "metadata": {},
   "source": [
    "## 3. Self‑guided exercise: Anaemia dataset\n",
    "\n",
    "The previous sections demonstrated how to perform supervised classification and unsupervised clustering using the iris data. Now it's your turn to apply these techniques to a realistic dataset that measures haematological indicators associated with anaemia.\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "This dataset is provided by (and can be cited as):\n",
    "\n",
    "> Mojumdar, Mayen Uddin ; Sarker, Dhiman; Assaduzzaman, Md; Rahman, Md. Mohaimenur; Sajeeb, Md. Anisul Haque; Bari , Md Shadikul ; Siddiquee, Shah Md Tanvir; Chakraborty, Narayan Ranjan (2024), “Pediatric Anemia Dataset: Hematological Indicators and Diagnostic Classification”, Mendeley Data, V1, doi: 10.17632/y7v7ff3wpj.1\n",
    "\n",
    "It can be downloaded from: https://data.mendeley.com/datasets/y7v7ff3wpj/1\n",
    "Since it is an Excel file (not a CSV) we also need a Python library like `openpyxl` to deal with it.\n",
    "\n",
    "To do all that in a terminal:\n",
    "\n",
    "```bash\n",
    "wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/y7v7ff3wpj-1.zip\n",
    "unzip y7v7ff3wpj-1.zip\n",
    "pip install openpyxl\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The anaemia dataset contains samples from patients with attributes such as:\n",
    "\n",
    "- **Gender** (categorical)\n",
    "- **Age** (in years)\n",
    "- **Hb**: haemoglobin (g/dL)\n",
    "- **RBC**: red blood cell count (million/µL)\n",
    "- **PCV**: packed cell volume (%)\n",
    "- **MCV**: mean corpuscular volume (fL)\n",
    "- **MCH**: mean corpuscular haemoglobin (pg)\n",
    "- **MCHC**: mean corpuscular haemoglobin concentration (g/dL)\n",
    "- **Decision_Class**: target variable indicating anaemia classification (1 for anaemic, 0 for healthy)\n",
    "\n",
    "These parameters were collected to support **predictive models and early diagnosis of anaemia**.\n",
    "\n",
    "### Task\n",
    "\n",
    "Use the code below as a starting point to build a classifier that predicts the anaemia status. You will need to:\n",
    "\n",
    "1. **Load the dataset** from a CSV file into a pandas DataFrame. If the file is named `anaemia.csv` and located in the same directory as this notebook, you can read it using `pd.read_csv('anaemia.csv')`.\n",
    "2. **Explore the data** – check for missing values, visualise distributions, and look for correlations.\n",
    "3. **Pre‑process the data** – convert the `Gender` column to numeric form (e.g. map 'm'→1, 'f'→0), and split the data into features `X_ana` and target `y_ana`.\n",
    "4. **Train a classifier** – you can start with a `RandomForestClassifier` as we used above. Split the data into training and testing sets, fit the model, and evaluate it using accuracy, precision, recall and the confusion matrix.\n",
    "5. **(Optional) Experiment** with other algorithms such as logistic regression or support vector machines, perform cross‑validation, and adjust hyperparameters.\n",
    "\n",
    "As you work through this exercise, refer back to the code from the Iris example and adapt it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8a43b-8919-48ec-ac48-4b60219fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "anaemia_df = pd.read_excel('Pediatric Anemia Dataset Hematological Indicators and Diagnostic Classification/Anemia Dataset.xlsx')\n",
    "anaemia_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5017e6f-e0dc-4062-8dad-1abce56861ef",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "Some suggested steps and code snippets are given. Try to write new code to perform each task. Go further if you like!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a894c-e94d-4be3-94df-bfd009fa8049",
   "metadata": {},
   "source": [
    "### 3.1  Understand the basic info about the data:\n",
    "```python\n",
    "anaemia_df.head()\n",
    "anaemia_df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbf27b-07f0-4a2f-952a-31fd95514771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01c197-cf09-4d27-b677-41475475aa57",
   "metadata": {},
   "source": [
    "### 3.2 Encode categories\n",
    "Often if it better to \"encode\" (i.e. translate) categorical feature data into a number.\n",
    "This is basically because the features are not going to be treated as binary labels, but rather like slightly fuzzy measurements.\n",
    "You could do something like: \n",
    "```python\n",
    "anaemia_df['Gender'] = anaemia_df['Gender'].map({'m': 1, 'f': 0})\n",
    "anaemia_df\n",
    "```\n",
    "to overwrite the `Gender` column with numbers instead of text labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a51cc5-66b1-49d6-9196-3288b6cb8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "\n",
    "# anaemia_df['Gender'] = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57037827-147b-4083-8dee-bcfbf3663325",
   "metadata": {},
   "source": [
    "### 3.3 Separate features and target\n",
    "As before, we want a feature matrix `X` and a class/target vector `y` (the anemia decision class).\n",
    "The features are in the columns with labels: `feature_cols = ['Gender', 'Age', 'Hb', 'RBC', 'PCV', 'MCV', 'MCH', 'MCHC']`\n",
    "\n",
    "And a column / list of columns can be selected from a dataframe like this: `my_column_series = df[\"my_column\"]` or `df_slice = df[[\"my_first_column\", \"my_second_column\"]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdea337-a5f4-4ed7-b196-6db27116e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "\n",
    "\n",
    "# feature_cols = ...\n",
    "# X = ...\n",
    "# y = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f8155-3ffa-4998-9142-665b4ef031fe",
   "metadata": {},
   "source": [
    "### 3.4 Make a test/train split of the data\n",
    "Remember we've got the `train_test_split(...)` method from sklearn to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fe68a-2acc-4699-8c3e-6c3339341cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code ...\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151191a-cbb9-4511-b449-ced6705d65b8",
   "metadata": {},
   "source": [
    "### 3.5 Make a Random Forest Classifier\n",
    "We want to train/fit a model (on the `train` subset) to classify the measurement as anaemic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4b4a7-05ce-43bc-97ae-7b6b8c167d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "\n",
    "# classifier = ...\n",
    "# classifier.fit(...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4abb0-4105-4dbd-9ccd-9dad47e810d9",
   "metadata": {},
   "source": [
    "### 3.6 Evaluate the model using the `test` subset\n",
    "- Use the `classifier` to `predict` the anaemia decision class of the `test` subset.\n",
    "- Look at the `classification_report` and `confusion_matrix` of those predictions vs the ground truth (`y_test`)\n",
    "- Visualise the confusion matrix (you can use Seaborn's `sns.heatmap` as we did for the Iris dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9f46d-8953-4a1d-91b3-b8133d6157fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "\n",
    "\n",
    "# y_pred = classifier.predict(...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035e372-ca28-49eb-8382-c83163804efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code...\n",
    "# cm = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c86a30",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background-color:#e6f3ff;\n",
    "  border-left:6px solid #2196F3;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    "  Bonus! Consider changing how many trees are used in the forest, or use one of `sklearn`'s other classifier like a `LogisticRegression`. You could also look for natural subgroups within the patients using k-means clustering... how would you work out the real-world relevance of any clusters?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fccec4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have:\n",
    "\n",
    "- Used `pandas` and `seaborn` to get data into Python, and visualise data out. \n",
    "- Built a supervised classification model (random forest) on the iris dataset (flowers), examined the confusion matrix and cross‑validated accuracy of the model by splitting up the data repeatedly.\n",
    "- Applied k‑means clustering to try and discover structure in unlabelled data.\n",
    "- Written your own code to apply all of those techniques to a dataset derived from pediatric anaemia patients.\n",
    "\n",
    "[Scikit Learn](https://scikit-learn.org/stable/), the library we have been using, is very widely used and popular, especially in tutorial settings like this. There are various other libraries too, including ones more suitable for vast datasets, for very complex models, or for natural language processing (e.g. [spacy](https://spacy.io/), which is similarly friendly to non-machine-learning-engineers).\n",
    "\n",
    "<div style=\"\n",
    "  background-color:#fdd757;\n",
    "  border-left:6px solid #ffb81c;\n",
    "  padding:10px 16px;\n",
    "  border-radius:6px;\n",
    "  font-size:14px;\n",
    "\">\n",
    " To use any of these techniques or tools for real use cases, you should carefully evaluate your methods, including deciding how you will evaluate them <b>before beginning to train and test</b>. This is important to avoid \"p-hacking\" where you try so many approaches that the fundamental assumptions required for the statistics to be valid are broken. Some research insistutions offer some form of statistics drop-in clinics to staff which, if available to you, are a great place to discuss the options for your own data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c232a-5c4a-4a01-9e82-da0d329b095f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
